diff -Naru a/arch/riscv/include/asm/mmu.h b/arch/riscv/include/asm/mmu.h
--- a/arch/riscv/include/asm/mmu.h	2020-04-29 22:33:25.000000000 +0800
+++ b/arch/riscv/include/asm/mmu.h	2020-04-30 23:18:12.237471000 +0800
@@ -17,6 +17,8 @@
 #endif
 } mm_context_t;
 
+void  update_mapping_prot(void);
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* _ASM_RISCV_MMU_H */
diff -Naru a/arch/riscv/include/asm/pgtable.h b/arch/riscv/include/asm/pgtable.h
--- a/arch/riscv/include/asm/pgtable.h	2020-04-29 22:33:25.000000000 +0800
+++ b/arch/riscv/include/asm/pgtable.h	2020-04-30 23:12:34.806778000 +0800
@@ -59,7 +59,8 @@
 
 #define PAGE_KERNEL		__pgprot(_PAGE_KERNEL)
 #define PAGE_KERNEL_EXEC	__pgprot(_PAGE_KERNEL | _PAGE_EXEC)
-
+#define PAGE_KERNEL_RO __pgprot(_PAGE_KERNEL & ~_PAGE_WRITE)
+#define PAGE_KERNEL_ROX __pgprot((_PAGE_KERNEL & ~_PAGE_WRITE)|_PAGE_EXEC)
 #define PAGE_TABLE		__pgprot(_PAGE_TABLE)
 
 extern pgd_t swapper_pg_dir[];
@@ -249,6 +250,11 @@
 	return __pte(pte_val(pte) & ~(_PAGE_WRITE));
 }
 
+static inline pte_t pte_execprotect(pte_t pte)
+{
+	return __pte(pte_val(pte) & ~(_PAGE_EXEC));
+}
+
 /* static inline pte_t pte_mkread(pte_t pte) */
 
 static inline pte_t pte_mkwrite(pte_t pte)
diff -Naru a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c
--- a/arch/riscv/mm/init.c	2020-04-29 22:33:25.000000000 +0800
+++ b/arch/riscv/mm/init.c	2020-04-30 23:18:33.224996000 +0800
@@ -18,6 +18,7 @@
 #include <asm/sections.h>
 #include <asm/pgtable.h>
 #include <asm/io.h>
+#include <asm/mmu.h>
 
 #include "../kernel/head.h"
 
@@ -409,7 +410,7 @@
 static void __init setup_vm_final(void)
 {
 	uintptr_t va, map_size;
-	phys_addr_t pa, start, end;
+	phys_addr_t pa, start, end, mid;
 	struct memblock_region *reg;
 
 	/* Set mmu_enabled flag */
@@ -424,7 +425,7 @@
 	for_each_memblock(memory, reg) {
 		start = reg->base;
 		end = start + reg->size;
-
+		mid = start + PMD_SIZE*3;
 		if (start >= end)
 			break;
 		if (memblock_is_nomap(reg))
@@ -434,10 +435,15 @@
 			start = __pa(PAGE_OFFSET);
 
 		map_size = best_map_size(start, end - start);
-		for (pa = start; pa < end; pa += map_size) {
+		for (pa = start; pa < mid; pa += PAGE_SIZE) {
+			va = (uintptr_t)__va(pa);
+			create_pgd_mapping(swapper_pg_dir, va, pa,
+					PAGE_SIZE, PAGE_KERNEL_EXEC);
+		}
+		for (pa = mid; pa < end; pa += map_size) {
 			va = (uintptr_t)__va(pa);
 			create_pgd_mapping(swapper_pg_dir, va, pa,
-					   map_size, PAGE_KERNEL_EXEC);
+					map_size, PAGE_KERNEL);
 		}
 	}
 
@@ -457,6 +463,7 @@
 	sparse_init();
 	setup_zero_page();
 	zone_sizes_init();
+	update_mapping_prot();
 }
 
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
diff -Naru a/arch/riscv/mm/Makefile b/arch/riscv/mm/Makefile
--- a/arch/riscv/mm/Makefile	2020-04-29 22:33:25.000000000 +0800
+++ b/arch/riscv/mm/Makefile	2020-04-30 15:53:25.935914000 +0800
@@ -12,6 +12,7 @@
 obj-y += cacheflush.o
 obj-y += context.o
 obj-y += sifive_l2_cache.o
+obj-y += mmu.o
 
 ifeq ($(CONFIG_MMU),y)
 obj-$(CONFIG_SMP) += tlbflush.o
diff -Naru a/arch/riscv/mm/mmu.c b/arch/riscv/mm/mmu.c
--- a/arch/riscv/mm/mmu.c	1970-01-01 08:00:00.000000000 +0800
+++ b/arch/riscv/mm/mmu.c	2020-04-30 15:55:00.029711000 +0800
@@ -0,0 +1,94 @@
+#include <linux/init.h>
+#include <linux/mm.h>
+#include <linux/memblock.h>
+#include <linux/initrd.h>
+#include <linux/swap.h>
+#include <linux/sizes.h>
+#include <linux/of_fdt.h>
+#include <linux/libfdt.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+
+#include <asm/fixmap.h>
+#include <asm/tlbflush.h>
+#include <asm/sections.h>
+#include <asm/pgtable.h>
+#include <asm/pgtable-64.h>
+#include <asm/page.h>
+#include <asm/io.h>
+#include <asm/pgtable-bits.h>
+
+static void  walk_pte(pmd_t *pmdp, unsigned long start, unsigned long end, pgprot_t prot)
+{
+        unsigned long addr = start;
+        pte_t *ptep = pte_offset_kernel(pmdp, start);
+        do {
+                pte_t pte  = *ptep;
+                if((prot.pgprot & ~(_PAGE_WRITE)) == prot.pgprot)
+                        pte = pte_wrprotect(pte);
+                if((prot.pgprot & ~(_PAGE_EXEC)) == prot.pgprot)
+                        pte = pte_execprotect(pte);
+                set_pte(ptep, pte);
+        } while (ptep++, addr += PAGE_SIZE, addr != end);
+}
+
+static void  walk_pmd(pud_t *pudp, unsigned long start, unsigned long end, pgprot_t prot)
+{
+        unsigned long next, addr = start;
+        unsigned long va;
+        pmd_t *pmdp = pmd_offset(pudp, start);
+
+        do {
+                pmd_t pmd = READ_ONCE(*pmdp);
+                next = pmd_addr_end(addr, end);
+
+                if (!pmd_none(pmd)) {
+                        BUG_ON(pmd_bad(pmd));
+                        walk_pte(pmdp, addr, next, prot);
+                }
+        } while (pmdp++, addr = next, addr != end);
+}
+
+static void  walk_pud(pgd_t *pgdp, unsigned long start, unsigned long end, pgprot_t prot)
+{
+        unsigned long next, addr = start;
+        pud_t *pudp = pud_offset((p4d_t *)pgdp, start);
+
+        do {
+                pud_t pud = READ_ONCE(*pudp);
+                next = pud_addr_end(addr, end);
+
+                if (!pud_none(pud)) {
+                        BUG_ON(pud_bad(pud));
+                        walk_pmd(pudp, addr, next, prot);
+                }
+        } while (pudp++, addr = next, addr != end);
+}
+
+static void   walk_pgd(struct mm_struct *mm, unsigned long start, unsigned long end,pgprot_t prot)
+{
+        start = ((start >> PAGE_SHIFT) + 1)<< PAGE_SHIFT;
+        end = (end >> PAGE_SHIFT) << PAGE_SHIFT;
+        unsigned long next, addr = start;
+        pgd_t *pgdp = pgd_offset(mm, start);
+
+        do {
+                pgd_t pgd = READ_ONCE(*pgdp);
+                next = pgd_addr_end(addr, end);
+
+                if (!pgd_none(pgd)) {
+                        BUG_ON(pgd_bad(pgd));
+                        walk_pud(pgdp, addr, next, prot);
+                }
+        } while (pgdp++, addr = next, addr != end);
+}
+
+void  update_mapping_prot()
+{
+
+        walk_pgd(&init_mm, (unsigned long)(_stext), (unsigned long)(_etext), PAGE_KERNEL_ROX);
+        walk_pgd(&init_mm,(unsigned long)(_sdata), (unsigned long)(_end), PAGE_KERNEL);
+        walk_pgd(&init_mm, (unsigned long)(__start_rodata), (unsigned long)(__start_ro_after_init), PAGE_KERNEL_RO);
+        walk_pgd(&init_mm, (unsigned long)(__end_ro_after_init), (unsigned long)(__end_rodata), PAGE_KERNEL_RO);
+	flush_tlb_all();
+}

